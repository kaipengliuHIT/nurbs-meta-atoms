{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NURBS Transformer Surrogate Model\n",
    "\n",
    "This notebook generates training data using Meep simulation and trains the Transformer surrogate model.\n",
    "\n",
    "**Note**: Make sure to activate the parallel Meep Python environment before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to get latest changes\n",
    "import importlib\n",
    "\n",
    "# Import and reload nurbs_atoms_data\n",
    "import nurbs_atoms_data\n",
    "importlib.reload(nurbs_atoms_data)\n",
    "from nurbs_atoms_data import Simulation\n",
    "\n",
    "# Import and reload transformer_nurbs_model\n",
    "import transformer_nurbs_model\n",
    "importlib.reload(transformer_nurbs_model)\n",
    "from transformer_nurbs_model import (\n",
    "    NURBSTransformerModel,\n",
    "    NURBSDataset,\n",
    "    normalize_control_points,\n",
    "    denormalize_control_points,\n",
    "    normalize_targets,\n",
    "    denormalize_targets\n",
    ")\n",
    "\n",
    "print(\"Modules imported and reloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Training Data\n",
    "\n",
    "Choose one of two methods:\n",
    "- **Method A**: Use Meep simulation (slow but realistic)\n",
    "- **Method B**: Use synthetic data (fast, for testing model architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_REAL_SIMULATION = False  # Set to True to use Meep simulation\n",
    "N_SAMPLES = 500  # Number of training samples\n",
    "WAVELENGTH = 550e-9  # 550 nm\n",
    "PERTURBATION_RANGE = 0.04  # Control point perturbation range\n",
    "\n",
    "# Base control points (default NURBS shape)\n",
    "BASE_CONTROL_POINTS = np.array([\n",
    "    (0.18, 0), (0.16, 0.16), (0, 0.18), (-0.16, 0.16),\n",
    "    (-0.18, 0), (-0.16, -0.16), (0, -0.16), (0.16, -0.16)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples, base_points, perturbation_range=0.04):\n",
    "    \"\"\"\n",
    "    Generate synthetic training data (for testing model architecture).\n",
    "    Creates a physics-inspired relationship between control points and optical response.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_samples} synthetic samples...\")\n",
    "    \n",
    "    control_points_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # Generate perturbed control points\n",
    "        perturbation = np.random.uniform(-perturbation_range, perturbation_range, (8, 2))\n",
    "        cp = base_points + perturbation\n",
    "        cp = np.clip(cp, -0.22, 0.22)\n",
    "        \n",
    "        # Calculate synthetic optical response (physics-inspired)\n",
    "        # Area affects transmittance, asymmetry affects phase\n",
    "        area = 0.5 * np.abs(np.sum(cp[:-1, 0] * np.roll(cp[:-1, 1], -1) - \n",
    "                                    np.roll(cp[:-1, 0], -1) * cp[:-1, 1]))\n",
    "        centroid_x = np.mean(cp[:, 0])\n",
    "        centroid_y = np.mean(cp[:, 1])\n",
    "        asymmetry = np.sqrt(centroid_x**2 + centroid_y**2)\n",
    "        \n",
    "        # Map to phase and transmittance with some noise\n",
    "        phase = np.arctan2(centroid_y, centroid_x) + np.random.normal(0, 0.1)\n",
    "        phase = np.clip(phase, -np.pi, np.pi)\n",
    "        \n",
    "        transmittance = 0.3 + 0.6 * (area / 0.05) + np.random.normal(0, 0.05)\n",
    "        transmittance = np.clip(transmittance, 0.1, 0.99)\n",
    "        \n",
    "        control_points_list.append(cp)\n",
    "        targets_list.append([phase, transmittance])\n",
    "    \n",
    "    return np.array(control_points_list), np.array(targets_list)\n",
    "\n",
    "\n",
    "def generate_simulation_data(n_samples, base_points, wavelength, perturbation_range=0.04):\n",
    "    \"\"\"\n",
    "    Generate training data using Meep simulation.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_samples} samples using Meep simulation...\")\n",
    "    print(\"This will take a while...\")\n",
    "    \n",
    "    control_points_list = []\n",
    "    targets_list = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # Generate perturbed control points\n",
    "        perturbation = np.random.uniform(-perturbation_range, perturbation_range, (8, 2))\n",
    "        cp = base_points + perturbation\n",
    "        cp = np.clip(cp, -0.22, 0.22)\n",
    "        \n",
    "        try:\n",
    "            # Run simulation\n",
    "            sim = Simulation(control_points=cp)\n",
    "            transmittance, phase = sim.run_forward(\n",
    "                wavelength_start=wavelength,\n",
    "                wavelength_stop=wavelength\n",
    "            )\n",
    "            \n",
    "            control_points_list.append(cp)\n",
    "            targets_list.append([phase, transmittance])\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            if failed_count <= 5:\n",
    "                print(f\"\\nSimulation {i} failed: {e}\")\n",
    "            # Use synthetic fallback\n",
    "            control_points_list.append(cp)\n",
    "            phase = np.random.uniform(-np.pi, np.pi)\n",
    "            trans = np.random.uniform(0.3, 0.9)\n",
    "            targets_list.append([phase, trans])\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"\\nTotal failed simulations: {failed_count}/{n_samples}\")\n",
    "    \n",
    "    return np.array(control_points_list), np.array(targets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data based on configuration\n",
    "if USE_REAL_SIMULATION:\n",
    "    control_points, targets = generate_simulation_data(\n",
    "        N_SAMPLES, BASE_CONTROL_POINTS, WAVELENGTH, PERTURBATION_RANGE\n",
    "    )\n",
    "else:\n",
    "    control_points, targets = generate_synthetic_data(\n",
    "        N_SAMPLES, BASE_CONTROL_POINTS, PERTURBATION_RANGE\n",
    "    )\n",
    "\n",
    "print(f\"\\nData shape: control_points={control_points.shape}, targets={targets.shape}\")\n",
    "print(f\"Phase range: [{targets[:, 0].min():.3f}, {targets[:, 0].max():.3f}]\")\n",
    "print(f\"Transmittance range: [{targets[:, 1].min():.3f}, {targets[:, 1].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data\n",
    "np.save('training_control_points.npy', control_points)\n",
    "np.save('training_targets.npy', targets)\n",
    "print(\"Raw data saved to training_control_points.npy and training_targets.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "normalized_control_points = normalize_control_points(control_points, min_val=-0.25, max_val=0.25)\n",
    "normalized_targets = normalize_targets(targets)\n",
    "\n",
    "print(f\"Normalized control points range: [{normalized_control_points.min():.3f}, {normalized_control_points.max():.3f}]\")\n",
    "print(f\"Normalized targets range: [{normalized_targets.min():.3f}, {normalized_targets.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation/test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    normalized_control_points, normalized_targets, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = NURBSDataset(X_train, y_train)\n",
    "val_dataset = NURBSDataset(X_val, y_val)\n",
    "test_dataset = NURBSDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = NURBSTransformerModel(\n",
    "    n_control_points=8,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    d_ff=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model created on device: {model.device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_input = torch.randn(2, 8, 2)  # batch_size=2, n_points=8, dim=2\n",
    "test_input = test_input.to(model.device)\n",
    "\n",
    "model.model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model.model(test_input)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output shape: {test_output.shape}\")\n",
    "    print(f\"Output values: {test_output}\")\n",
    "print(\"\\nForward pass test PASSED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "EPOCHS = 100\n",
    "MODEL_SAVE_PATH = \"nurbs_transformer_model.pth\"\n",
    "\n",
    "print(f\"Training for {EPOCHS} epochs...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model.train(train_loader, val_loader, epochs=EPOCHS, save_path=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(model.train_losses, label='Training Loss', linewidth=2)\n",
    "axes[0].plot(model.val_losses, label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Loss curves (linear scale, last 50%)\n",
    "start_idx = len(model.train_losses) // 2\n",
    "axes[1].plot(range(start_idx, len(model.train_losses)), \n",
    "             model.train_losses[start_idx:], label='Training Loss', linewidth=2)\n",
    "axes[1].plot(range(start_idx, len(model.val_losses)), \n",
    "             model.val_losses[start_idx:], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss (MSE)')\n",
    "axes[1].set_title('Loss (Last 50% of Training)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Training curves saved to training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "avg_loss, avg_phase_error, avg_trans_error = model.evaluate(test_loader)\n",
    "\n",
    "print(f\"\\n=== Test Set Evaluation ===\")\n",
    "print(f\"Average Loss (MSE): {avg_loss:.6f}\")\n",
    "print(f\"Average Phase Error (normalized): {avg_phase_error:.6f}\")\n",
    "print(f\"Average Transmittance Error (normalized): {avg_trans_error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set and compare\n",
    "model.model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_points, batch_targets in test_loader:\n",
    "        batch_points = batch_points.to(model.device)\n",
    "        outputs = model.model(batch_points)\n",
    "        all_predictions.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(batch_targets.numpy())\n",
    "\n",
    "predictions = np.array(all_predictions)\n",
    "actuals = np.array(all_targets)\n",
    "\n",
    "# Denormalize\n",
    "pred_denorm = denormalize_targets(predictions)\n",
    "actual_denorm = denormalize_targets(actuals)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Actuals shape: {actuals.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Phase\n",
    "axes[0].scatter(actual_denorm[:, 0], pred_denorm[:, 0], alpha=0.6, s=30)\n",
    "axes[0].plot([-np.pi, np.pi], [-np.pi, np.pi], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Phase (rad)')\n",
    "axes[0].set_ylabel('Predicted Phase (rad)')\n",
    "axes[0].set_title('Phase: Predicted vs Actual')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Transmittance\n",
    "axes[1].scatter(actual_denorm[:, 1], pred_denorm[:, 1], alpha=0.6, s=30)\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual Transmittance')\n",
    "axes[1].set_ylabel('Predicted Transmittance')\n",
    "axes[1].set_title('Transmittance: Predicted vs Actual')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Prediction comparison saved to prediction_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R² scores\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "phase_r2 = r2_score(actual_denorm[:, 0], pred_denorm[:, 0])\n",
    "trans_r2 = r2_score(actual_denorm[:, 1], pred_denorm[:, 1])\n",
    "\n",
    "phase_mae = mean_absolute_error(actual_denorm[:, 0], pred_denorm[:, 0])\n",
    "trans_mae = mean_absolute_error(actual_denorm[:, 1], pred_denorm[:, 1])\n",
    "\n",
    "print(f\"\\n=== Final Model Performance ===\")\n",
    "print(f\"Phase R² Score: {phase_r2:.4f}\")\n",
    "print(f\"Phase MAE: {phase_mae:.4f} rad ({np.degrees(phase_mae):.2f}°)\")\n",
    "print(f\"\\nTransmittance R² Score: {trans_r2:.4f}\")\n",
    "print(f\"Transmittance MAE: {trans_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction with a single sample\n",
    "test_cp = BASE_CONTROL_POINTS + np.random.uniform(-0.02, 0.02, (8, 2))\n",
    "test_cp = np.clip(test_cp, -0.22, 0.22)\n",
    "\n",
    "# Normalize\n",
    "test_cp_norm = normalize_control_points(test_cp.reshape(1, 8, 2), min_val=-0.25, max_val=0.25)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(test_cp_norm)\n",
    "pred_denorm = denormalize_targets(prediction)\n",
    "\n",
    "print(f\"Test control points:\\n{test_cp}\")\n",
    "print(f\"\\nPredicted phase: {pred_denorm[0, 0]:.4f} rad ({np.degrees(pred_denorm[0, 0]):.2f}°)\")\n",
    "print(f\"Predicted transmittance: {pred_denorm[0, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nModel saved to: {MODEL_SAVE_PATH}\")\n",
    "print(f\"Training data saved to: training_control_points.npy, training_targets.npy\")\n",
    "print(f\"\\nTo use the model for inference:\")\n",
    "print(\"  model = NURBSTransformerModel(...)\")\n",
    "print(f\"  model.load_model('{MODEL_SAVE_PATH}')\")\n",
    "print(\"  prediction = model.predict(control_points)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
